{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Scraping eBay Data Using Selenium\n",
    "\n",
    "This assignment will guide you through the steps required to scrape product data from [eBay](https://www.ebay.com/) using Selenium. Your goal is to collect data about products based on a specific search query and store the data in a CSV file for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Below is a step-by-step outline of the scraping process. Follow these steps and implement the required code to complete the assignment. Comment your code wherever necessary to explain your thought process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Set Up Selenium**\n",
    "1. Import the necessary modules from Selenium (e.g., `webdriver`, `By`, `Keys`, etc.).\n",
    "2. Set up the Chrome WebDriver to control the browser. Ensure you have downloaded the ChromeDriver executable and placed it in the correct directory.\n",
    "3. Navigate to the eBay homepage using the WebDriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "path = 'C:\\\\avast\\\\chromedriver.exe' \n",
    "driver = webdriver.Chrome(executable_path=path)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.ebay.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Perform a Search**\n",
    "1. Identify the search bar element on the eBay homepage using an appropriate locator (e.g., `id`, `name`, `XPath`).\n",
    "2. Send a specific search query (e.g., \"laptops\") to the search bar and simulate pressing the Enter key.\n",
    "3. Wait for the search results page to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver_path = 'path/to/chromedriver'  \n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "driver.get(\"https://www.ebay.com\")\n",
    "driver.maximize_window()\n",
    "\n",
    "try:\n",
    "    search_bar = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.NAME, \"_nkw\"))\n",
    "    )\n",
    "    search_query = \"laptops\"\n",
    "    search_bar.send_keys(search_query + Keys.RETURN)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.title_contains(search_query)\n",
    "    )\n",
    "    print(driver.title)\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Extract Product Data**\n",
    "1. Use `find_elements` to locate product titles, prices, and other relevant data on the search results page. For example:\n",
    "   - Product title: Locate elements displaying the product names.\n",
    "   - Price: Locate elements showing product prices.\n",
    "   - (Optional) Link: Extract the URL for each product.\n",
    "2. Loop through the extracted elements and store the data in a structured format (e.g., a Python list of dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "driver_path = 'path/to/chromedriver' \n",
    "driver = webdriver.Chrome(service=Service(executable_path=driver_path))\n",
    "\n",
    "driver.get(\"https://www.ebay.com\")\n",
    "driver.maximize_window()\n",
    "\n",
    "try:\n",
    "    search_bar = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.NAME, \"_nkw\"))\n",
    "    )\n",
    "    \n",
    "    search_query = \"laptops\"\n",
    "    search_bar.send_keys(search_query + Keys.RETURN)\n",
    "\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.title_contains(search_query)\n",
    "    )\n",
    "    products = []\n",
    "    product_elements = driver.find_elements(By.CSS_SELECTOR, \".s-item\")  \n",
    "\n",
    "    for item in product_elements:\n",
    "        title_element = item.find_element(By.CSS_SELECTOR, \".s-item__title\")\n",
    "        price_element = item.find_element(By.CSS_SELECTOR, \".s-item__price\")\n",
    "        link_element = item.find_element(By.CSS_SELECTOR, \".s-item__link\")\n",
    "\n",
    "        product_data = {\n",
    "            \"title\": title_element.text,\n",
    "            \"price\": price_element.text,\n",
    "            \"url\": link_element.get_attribute(\"href\")\n",
    "        }\n",
    "        products.append(product_data)\n",
    "\n",
    "    for product in products:\n",
    "        print(product)\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Handle Pagination**\n",
    "1. Check for the presence of a \"Next\" button to navigate to the next page of results.\n",
    "2. Implement a loop to scrape multiple pages of search results. Break the loop when no more pages are available or after a set number of pages (e.g., 5 pages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "driver_path = 'path/to/chromedriver' \n",
    "service = Service(executable_path=driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(\"https://www.ebay.com\")\n",
    "driver.maximize_window()\n",
    "\n",
    "try:\n",
    "    search_bar = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.NAME, \"_nkw\"))\n",
    "    )\n",
    "    search_query = \"laptops\"\n",
    "    search_bar.send_keys(search_query + Keys.RETURN)\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Save Data to CSV**\n",
    "1. Use the `pandas` library to convert the scraped data into a DataFrame.\n",
    "2. Save the DataFrame to a CSV file with appropriate column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd  \n",
    "driver_path = 'path/to/chromedriver'\n",
    "service = Service(executable_path=driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(\"https://www.ebay.com\")\n",
    "driver.maximize_window()\n",
    "\n",
    "try:\n",
    "    search_bar = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.NAME, \"_nkw\"))\n",
    "    )\n",
    "\n",
    "    search_query = \"laptops\"\n",
    "    search_bar.send_keys(search_query + Keys.RETURN)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.title_contains(search_query)\n",
    "    )\n",
    "    products = []\n",
    "    max_pages = 5  \n",
    "    current_page = 1\n",
    "\n",
    "    while current_page <= max_pages:\n",
    "        product_elements = driver.find_elements(By.CSS_SELECTOR, \".s-item\")  \n",
    "\n",
    "        for item in product_elements:\n",
    "            try:\n",
    "                title_element = item.find_element(By.CSS_SELECTOR, \".s-item__title\")\n",
    "                price_element = item.find_element(By.CSS_SELECTOR, \".s-item__price\")\n",
    "                link_element = item.find_element(By.CSS_SELECTOR, \".s-item__link\")\n",
    "\n",
    "                product_data = {\n",
    "                    \"title\": title_element.text,\n",
    "                    \"price\": price_element.text,\n",
    "                    \"url\": link_element.get_attribute(\"href\")\n",
    "                }\n",
    "                products.append(product_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data from item: {e}\")\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \".pagination__next\")\n",
    "            if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                break  \n",
    "            else:\n",
    "                next_button.click() \n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.title_contains(search_query) \n",
    "                )\n",
    "                current_page += 1  \n",
    "        except Exception as e:\n",
    "            print(f\"No next button found or error: {e}\")\n",
    "            break  \n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv('ebay_products.csv', index=False, header=True)\n",
    "    print(\"Data has been saved to ebay_products.csv\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Close the Browser**\n",
    "1. Once the scraping is complete, ensure the WebDriver is closed to release system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "\n",
    "Run\n",
    "\n",
    "Copy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd  \n",
    "driver_path = 'path/to/chromedriver' \n",
    "service = Service(executable_path=driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(\"https://www.ebay.com\")\n",
    "driver.maximize_window()\n",
    "\n",
    "try:\n",
    "    search_bar = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.NAME, \"_nkw\"))\n",
    "    )\n",
    "    search_query = \"laptops\"\n",
    "    search_bar.send_keys(search_query + Keys.RETURN)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.title_contains(search_query)\n",
    "    )\n",
    "    products = []\n",
    "    max_pages = 5 \n",
    "    current_page = 1\n",
    "\n",
    "    while current_page <= max_pages:\n",
    "        product_elements = driver.find_elements(By.CSS_SELECTOR, \".s-item\")\n",
    "\n",
    "        for item in product_elements:\n",
    "            try:\n",
    "                title_element = item.find_element(By.CSS_SELECTOR, \".s-item__title\")\n",
    "                price_element = item.find_element(By.CSS_SELECTOR, \".s-item__price\")\n",
    "                link_element = item.find_element(By.CSS_SELECTOR, \".s-item__link\")\n",
    "\n",
    "                product_data = {\n",
    "                    \"title\": title_element.text,\n",
    "                    \"price\": price_element.text,\n",
    "                    \"url\": link_element.get_attribute(\"href\")\n",
    "                }\n",
    "                products.append(product_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting data from item: {e}\")\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \".pagination__next\")\n",
    "            if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                break \n",
    "            else:\n",
    "                next_button.click()  \n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.title_contains(search_query) \n",
    "                )\n",
    "                current_page += 1\n",
    "        except Exception as e:\n",
    "            print(f\"No next button found or error: {e}\")\n",
    "            break  \n",
    "\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv('ebay_products.csv', index=False, header=True)\n",
    "    print(\"Data has been saved to ebay_products.csv\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deliverables**\n",
    "- Submit the Python script you implemented on your github, following the above steps.\n",
    "- Ensure that your script:\n",
    "  - Extracts data for at least 50 products.\n",
    "  - Includes product titles, prices, and links (if applicable).\n",
    "  - Saves the data to a CSV file named `ebay_products.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bonus Challenge**\n",
    "1. Add functionality to scrape product ratings and the number of reviews (if available).\n",
    "2. Include error handling to skip elements that might be missing data or inaccessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good luck!** 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
